{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10b21e33-5ce1-4ac0-8d75-fcd270425986",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Expect all shape elements to be an integer, actual type: (<class 'NoneType'>, <class 'int'>)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01menv\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menv\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KeibaEnv\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# ゲーム環境を作成します\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mKeibaEnv\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# ゲーム環境を初期化します。\u001b[39;00m\n\u001b[0;32m     17\u001b[0m observation \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n",
      "File \u001b[1;32mC:\\umejima\\work\\visualpaper\\keiba\\keiba-learning\\src\\env\\env.py:16\u001b[0m, in \u001b[0;36mKeibaEnv.__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# とりうる Action の数\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# 0: xx\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# 1: yy\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mspaces\u001b[38;5;241m.\u001b[39mDiscrete(\u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation_space \u001b[38;5;241m=\u001b[39m \u001b[43mgym\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspaces\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBox\u001b[49m\u001b[43m(\u001b[49m\u001b[43m    \u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlow\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m10000.\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhigh\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10000.\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m49\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# おそらく 50 列、20 行持つ... のだと思う\u001b[39;49;00m\n\u001b[0;32m     20\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreward_range \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100.\u001b[39m, \u001b[38;5;241m100.\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\keiba_ai_learning2\\lib\\site-packages\\gym\\spaces\\box.py:88\u001b[0m, in \u001b[0;36mBox.__init__\u001b[1;34m(self, low, high, shape, dtype, seed)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;66;03m# determine shape if it isn't provided directly\u001b[39;00m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 88\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\n\u001b[0;32m     89\u001b[0m         np\u001b[38;5;241m.\u001b[39missubdtype(\u001b[38;5;28mtype\u001b[39m(dim), np\u001b[38;5;241m.\u001b[39minteger) \u001b[38;5;28;01mfor\u001b[39;00m dim \u001b[38;5;129;01min\u001b[39;00m shape\n\u001b[0;32m     90\u001b[0m     ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpect all shape elements to be an integer, actual type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mtype\u001b[39m(dim) \u001b[38;5;28;01mfor\u001b[39;00m dim \u001b[38;5;129;01min\u001b[39;00m shape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     91\u001b[0m     shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mint\u001b[39m(dim) \u001b[38;5;28;01mfor\u001b[39;00m dim \u001b[38;5;129;01min\u001b[39;00m shape)  \u001b[38;5;66;03m# This changes any np types to int\u001b[39;00m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(low, np\u001b[38;5;241m.\u001b[39mndarray):\n",
      "\u001b[1;31mAssertionError\u001b[0m: Expect all shape elements to be an integer, actual type: (<class 'NoneType'>, <class 'int'>)"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from env.env import KeibaEnv\n",
    "\n",
    "# ゲーム環境を作成します\n",
    "env = KeibaEnv()\n",
    "\n",
    "# ゲーム環境を初期化します。\n",
    "observation = env.reset()\n",
    "\n",
    "# 環境からアクション数を取得します。このゲームでは4となります。\n",
    "nb_actions = env.action_space.n\n",
    "\n",
    "# Kerasを使ってモデルを作成します。\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(1,) + env.observation_space.shape),\n",
    "    keras.layers.Dense(16, activation=\"relu\"),\n",
    "    keras.layers.Dense(16, activation=\"relu\"),\n",
    "    keras.layers.Dense(nb_actions, activation=\"linear\"),\n",
    "])\n",
    "\n",
    "# 経験値を蓄積するためのメモリです。学習を安定させるために使用します。\n",
    "memory = SequentialMemory(limit=50000, window_length=1)\n",
    "\n",
    "# 行動ポリシーはBoltzmannQPolicyを使用しています。\n",
    "# EpsGreedyQPolicyと比較して、こちらの方が収束が早かったので採用しています。\n",
    "policy = BoltzmannQPolicy()\n",
    "\n",
    "# DQNAgentを作成します。\n",
    "dqn = DQNAgent(\n",
    "    model=model,\n",
    "    nb_actions=nb_actions,\n",
    "    memory=memory,\n",
    "    target_model_update=1e-2,\n",
    "    policy=policy)\n",
    "\n",
    "# DQNAgentのコンパイル。最適化はAdam,評価関数はMAEを使用します。\n",
    "dqn.compile(Adam(learning_rate=1e-3), metrics=['mae'])\n",
    "\n",
    "# 学習を開始します。100000ステップ実行します。\n",
    "history = dqn.fit(env, nb_steps=100, visualize=False, verbose=1)\n",
    "\n",
    "# 学習した重みをファイルに保存します。\n",
    "dqn.save_weights('moving_test.hdf5')\n",
    "\n",
    "# ゲームごとのステップ数と報酬をグラフ化します。\n",
    "plt.plot(history.history['nb_episode_steps'], label='nb_episode_steps')\n",
    "plt.plot(history.history['episode_reward'], label='episode_reward')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7fd976-9d93-4247-b4b1-f18d397f2da1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "4caf82e643e4ad2a990ff255ea5c6c47b964940d4c625cd8ad41c21257366cb4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
