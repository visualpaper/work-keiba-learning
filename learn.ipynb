{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0e48a61-a942-4c98-a427-76d6a2c0d40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "\n",
    "class MovingEnv(gym.Env):\n",
    "    ACTION_NUM = 4\n",
    "    MIN_POS = 0\n",
    "    MAX_POS = 99\n",
    "    MAX_STEPS = 100\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.action_space = gym.spaces.Discrete(self.ACTION_NUM)\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=np.array([self.MIN_POS, self.MIN_POS]),\n",
    "            high=np.array([self.MAX_POS, self.MAX_POS]),\n",
    "            dtype=np.int16)\n",
    "\n",
    "    def reset(self):\n",
    "        self.pos = np.random.randint(self.MIN_POS, self.MAX_POS + 1)\n",
    "        self.goal = np.random.randint(self.MIN_POS, self.MAX_POS + 1)\n",
    "        self.steps = 0\n",
    "        return np.array([self.goal, self.pos])\n",
    "\n",
    "    def step(self, action):\n",
    "        '''\n",
    "        action\n",
    "        0: -10移動\n",
    "        1: -1移動\n",
    "        2: +1移動\n",
    "        3: +10移動\n",
    "        '''\n",
    "        if action == 0:\n",
    "            next_pos = self.pos - 10\n",
    "        elif action == 1:\n",
    "            next_pos = self.pos - 1\n",
    "        elif action == 2:\n",
    "            next_pos = self.pos + 1\n",
    "        elif action == 3:\n",
    "            next_pos = self.pos + 10\n",
    "        else:\n",
    "            next_pos = self.pos\n",
    "\n",
    "        if next_pos < self.MIN_POS:\n",
    "            next_pos = self.MIN_POS\n",
    "        elif next_pos > self.MAX_POS:\n",
    "            next_pos = self.MAX_POS\n",
    "\n",
    "        self.pos = next_pos\n",
    "        self.steps += 1\n",
    "        reward = 100 if self.pos == self.goal else -1\n",
    "        done = True if self.pos == self.goal or self.steps > self.MAX_STEPS else False\n",
    "\n",
    "        return np.array([self.goal, self.pos]), reward, done, {}\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        a = ['.' for x in range(self.MIN_POS, self.MAX_POS + 1)]\n",
    "        a[self.goal] = 'G'\n",
    "        a[self.pos] = 'o'\n",
    "        print(f'\\r{\"\".join(a)}', end='')\n",
    "        time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f2a75c-5c53-4cc4-b7a1-5ca63dfa4789",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.envs.registration import register\n",
    "\n",
    "register(\n",
    "    id='movingenv-v0',\n",
    "    entry_point='.:MovingEnv',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b21e33-5ce1-4ac0-8d75-fcd270425986",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MovingEnv' object has no attribute 'observation_space'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 20\u001b[0m\n\u001b[0;32m     16\u001b[0m nb_actions \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mn\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Kerasを使ってモデルを作成します。\u001b[39;00m\n\u001b[0;32m     19\u001b[0m model \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mSequential([\n\u001b[1;32m---> 20\u001b[0m     keras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mFlatten(input_shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m,) \u001b[38;5;241m+\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservation_space\u001b[49m\u001b[38;5;241m.\u001b[39mshape),\n\u001b[0;32m     21\u001b[0m     keras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDense(\u001b[38;5;241m16\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     22\u001b[0m     keras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDense(\u001b[38;5;241m16\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     23\u001b[0m     keras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDense(nb_actions, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     24\u001b[0m ])\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# 経験値を蓄積するためのメモリです。学習を安定させるために使用します。\u001b[39;00m\n\u001b[0;32m     27\u001b[0m memory \u001b[38;5;241m=\u001b[39m SequentialMemory(limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50000\u001b[39m, window_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'MovingEnv' object has no attribute 'observation_space'"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ゲーム環境を作成します\n",
    "env = MovingEnv()\n",
    "\n",
    "# ゲーム環境を初期化します。\n",
    "observation = env.reset()\n",
    "\n",
    "# 環境からアクション数を取得します。このゲームでは4となります。\n",
    "nb_actions = env.action_space.n\n",
    "\n",
    "# Kerasを使ってモデルを作成します。\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(1,) + env.observation_space.shape),\n",
    "    keras.layers.Dense(16, activation=\"relu\"),\n",
    "    keras.layers.Dense(16, activation=\"relu\"),\n",
    "    keras.layers.Dense(nb_actions, activation=\"linear\"),\n",
    "])\n",
    "\n",
    "# 経験値を蓄積するためのメモリです。学習を安定させるために使用します。\n",
    "memory = SequentialMemory(limit=50000, window_length=1)\n",
    "\n",
    "# 行動ポリシーはBoltzmannQPolicyを使用しています。\n",
    "# EpsGreedyQPolicyと比較して、こちらの方が収束が早かったので採用しています。\n",
    "policy = BoltzmannQPolicy()\n",
    "\n",
    "# DQNAgentを作成します。\n",
    "dqn = DQNAgent(\n",
    "    model=model,\n",
    "    nb_actions=nb_actions,\n",
    "    memory=memory,\n",
    "    target_model_update=1e-2,\n",
    "    policy=policy)\n",
    "\n",
    "# DQNAgentのコンパイル。最適化はAdam,評価関数はMAEを使用します。\n",
    "dqn.compile(Adam(learning_rate=1e-3), metrics=['mae'])\n",
    "\n",
    "# 学習を開始します。100000ステップ実行します。\n",
    "history = dqn.fit(env, nb_steps=100000, visualize=False, verbose=1)\n",
    "\n",
    "# 学習した重みをファイルに保存します。\n",
    "dqn.save_weights('moving_test.hdf5')\n",
    "\n",
    "# ゲームごとのステップ数と報酬をグラフ化します。\n",
    "plt.plot(history.history['nb_episode_steps'], label='nb_episode_steps')\n",
    "plt.plot(history.history['episode_reward'], label='episode_reward')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcafcef-2dc0-4faf-baae-4e9be4d56907",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ゲーム環境を作成します\n",
    "env = MovingEnv()\n",
    "\n",
    "# ゲーム環境を初期化します。\n",
    "observation = env.reset()\n",
    "\n",
    "nb_actions = env.action_space.n\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(1,) + env.observation_space.shape),\n",
    "    keras.layers.Dense(16, activation=\"relu\"),\n",
    "    keras.layers.Dense(16, activation=\"relu\"),\n",
    "    keras.layers.Dense(nb_actions, activation=\"linear\"),\n",
    "])\n",
    "\n",
    "memory = SequentialMemory(limit=50000, window_length=1)\n",
    "policy = BoltzmannQPolicy()\n",
    "dqn = DQNAgent(\n",
    "    model=model,\n",
    "    nb_actions=nb_actions,\n",
    "    memory=memory,\n",
    "    target_model_update=1e-2,\n",
    "    policy=policy)\n",
    "\n",
    "dqn.compile(Adam(learning_rate=1e-3), metrics=['mae'])\n",
    "\n",
    "# ↑ここまでは強化学習と同じソースです。\n",
    "\n",
    "# 保存した重みを読み込みます。\n",
    "dqn.load_weights('moving_test.hdf5')\n",
    "\n",
    "# 5エピソード(ゲーム）実行します。\n",
    "dqn.test(env, nb_episodes=5)\n",
    "#print(dqn.forward(observation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7fd976-9d93-4247-b4b1-f18d397f2da1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e11f7bf-9701-4130-84dc-5a3309ffd12d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "4caf82e643e4ad2a990ff255ea5c6c47b964940d4c625cd8ad41c21257366cb4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
